{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/lightgbm/__init__.py:48: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gc\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import os \n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_selection import RFECV\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#from Tuning_parameter import Lgb_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EDA.ipynb',\n",
       " 'try.csv',\n",
       " 'dataset_description.pdf',\n",
       " 'Tuning_parameter.py',\n",
       " '.DS_Store',\n",
       " 'Record',\n",
       " 'testing_set_prep.csv',\n",
       " 'Attachment',\n",
       " 'test.csv',\n",
       " 'No_cv.csv',\n",
       " '__pycache__',\n",
       " 'Image',\n",
       " 'Threshold.py',\n",
       " 'Model.py',\n",
       " 'README.md',\n",
       " 'REF.ipynb',\n",
       " 'Preprocessing.py',\n",
       " 'Andy.ipynb',\n",
       " 'train.csv',\n",
       " '.ipynb_checkpoints',\n",
       " '.vscode',\n",
       " 'Main.py',\n",
       " 'training_set_prep.csv',\n",
       " 'Model.ipynb',\n",
       " 'test.ipynb',\n",
       " 'Submission',\n",
       " 'Fe_selection.py']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "472"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('training_set_prep.csv')\n",
    "test = pd.read_csv('testing_set_prep.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reduce_mem(df):\n",
    "    start_mem_usg = df.memory_usage().sum() / (1024*1024)\n",
    "    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n",
    "    for col in df.keys():\n",
    "        if df[col].dtype == int:\n",
    "            Max = df[col].max()\n",
    "            Min = df[col].min()\n",
    "            if -128 < Min and Max < 127:\n",
    "                df[col] = df[col].astype(np.int8)\n",
    "            elif -32768 < Min and Max < 32767:\n",
    "                df[col] = df[col].astype(np.int16)\n",
    "            elif -2147483648 < Min and Max < 2147483647:\n",
    "                df[col] = df[col].astype(np.int32)\n",
    "            else:\n",
    "                df[col] = df[col].astype(np.int)\n",
    "        elif df[col].dtype == float:\n",
    "            df[col] = df[col].astype(np.float32)\n",
    "        else:\n",
    "            continue\n",
    "    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n",
    "    mem_usg = df.memory_usage().sum() / 1024**2 \n",
    "    print(\"Memory usage is: \",mem_usg,\" MB\")\n",
    "    print(\"This is \",100*mem_usg/start_mem_usg,\"% of the initial size\")\n",
    "    return df\n",
    "\n",
    "def Process_fre(df, cols):\n",
    "    print('Initial Process_fre.....')\n",
    "    for col in cols:\n",
    "        vc = df[col].value_counts(dropna=True, normalize=True).to_dict()\n",
    "        vc[-1] = -1\n",
    "        new_col = col+'_freq'\n",
    "        df[col] = df[col].map(vc)\n",
    "        df[col] = df[col].astype('float32')\n",
    "\n",
    "    return df\n",
    "\n",
    "def Separate_dt(df1):   \n",
    "    df1['Month'] = 0\n",
    "    df1.loc[df1['locdt'] < 121, 'Month'] = 4\n",
    "    df1.loc[df1['locdt'] < 91, 'Month']  = 3\n",
    "    df1.loc[df1['locdt'] < 61, 'Month']  = 2\n",
    "    df1.loc[df1['locdt'] < 31, 'Month'] = 1\n",
    "    \n",
    "    return df1\n",
    "\n",
    "def Split_group_kfolds(df):\n",
    "    Train_X = df.drop(['fraud_ind'], axis=1)\n",
    "    Train_Y = df['fraud_ind']\n",
    "    Folds  = GroupKFold(n_splits=3)\n",
    "    Splited_data = Folds.split(Train_X, Train_Y, groups=Train_X['Month']) \n",
    "    return Splited_data\n",
    "\n",
    "def Cal_f1_score(True_y, Pre_y):\n",
    "    return f1_score(True_y, Pre_y)\n",
    "\n",
    "def F1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of properties dataframe is : 673.3983001708984  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  238.01154708862305  MB\n",
      "This is  35.344839306576695 % of the initial size\n",
      "Memory usage of properties dataframe is : 183.37189483642578  MB\n",
      "___MEMORY USAGE AFTER COMPLETION:___\n",
      "Memory usage is:  65.54748821258545  MB\n",
      "This is  35.74565680910705 % of the initial size\n"
     ]
    }
   ],
   "source": [
    "train = Separate_dt(train)\n",
    "train = Reduce_mem(train)\n",
    "test = Reduce_mem(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Feature :  42\n"
     ]
    }
   ],
   "source": [
    "Splited_data = Split_group_kfolds(train)\n",
    "Excluded_cols = ['bacno','cano','locdt', 'loctm','OOOO_tag','scity','cano_max_locdt','cano_min_locdt',\n",
    "            'cano_stocn_nunique_locdt','cano_stocn_diff_locdt','cano_stocn_min_diff_locdt', 'txkey',\n",
    "            'cano_stocn_min_locdt','cano_stocn_max_locdt', 'fraud_ind', 'Month', 'is_test']\n",
    "\n",
    "All_Features = [Feat for Feat in train.columns if Feat not in Excluded_cols]\n",
    "print('Total Feature : ', len(All_Features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lgb_params = {\n",
    "        'objective': 'binary',\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        'bagging_freq': 4,\n",
    "#         'num_boost_round' : 100, \n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Process_fre.....\n",
      "Initial Process_fre.....\n"
     ]
    }
   ],
   "source": [
    "Cate_feat = ['contp','mchno','acqic','mcc','insfg','stocn','stscd','iterm',\n",
    "             'ovrlt','flbmk','hcefg','flg_3dsmk','ecfg','etymd',\n",
    "             'cano_diff','change_signal','cano_diff_change_signal','stocn_scity']\n",
    "\n",
    "# train[Cate_feat] = train[Cate_feat].astype('category')\n",
    "# test[Cate_feat] = test[Cate_feat].astype('category')\n",
    "train = Process_fre(train, Cate_feat)\n",
    "test  = Process_fre(test, Cate_feat)\n",
    "for col in ['flbmk', 'flg_3dsmk', '14_variables_count']:\n",
    "    train[col].fillna(-999, inplace = True)\n",
    "    test[col].fillna(-999, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$1 fold\n",
      "Hold out  1 month\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0339617\ttraining's f1: 0.404617\tvalid_1's binary_logloss: 0.0327042\tvalid_1's f1: 0.421497\n",
      "[40]\ttraining's binary_logloss: 0.0280788\ttraining's f1: 0.547163\tvalid_1's binary_logloss: 0.0270394\tvalid_1's f1: 0.518628\n",
      "[60]\ttraining's binary_logloss: 0.0253716\ttraining's f1: 0.601903\tvalid_1's binary_logloss: 0.0249318\tvalid_1's f1: 0.554385\n",
      "[80]\ttraining's binary_logloss: 0.0237126\ttraining's f1: 0.634247\tvalid_1's binary_logloss: 0.0238089\tvalid_1's f1: 0.574836\n",
      "[100]\ttraining's binary_logloss: 0.0225746\ttraining's f1: 0.65831\tvalid_1's binary_logloss: 0.0233004\tvalid_1's f1: 0.587832\n",
      "[120]\ttraining's binary_logloss: 0.0216979\ttraining's f1: 0.668941\tvalid_1's binary_logloss: 0.023489\tvalid_1's f1: 0.590605\n",
      "[140]\ttraining's binary_logloss: 0.0212896\ttraining's f1: 0.673023\tvalid_1's binary_logloss: 0.0237984\tvalid_1's f1: 0.590673\n",
      "[160]\ttraining's binary_logloss: 0.0239646\ttraining's f1: 0.673961\tvalid_1's binary_logloss: 0.0312442\tvalid_1's f1: 0.589534\n",
      "Early stopping, best iteration is:\n",
      "[116]\ttraining's binary_logloss: 0.0218781\ttraining's f1: 0.667664\tvalid_1's binary_logloss: 0.023237\tvalid_1's f1: 0.592658\n",
      "$2 fold\n",
      "Hold out  3 month\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0306615\ttraining's f1: 0.530342\tvalid_1's binary_logloss: 0.0423273\tvalid_1's f1: 0.370388\n",
      "[40]\ttraining's binary_logloss: 0.0246312\ttraining's f1: 0.658378\tvalid_1's binary_logloss: 0.042679\tvalid_1's f1: 0.419431\n",
      "[60]\ttraining's binary_logloss: 0.0220388\ttraining's f1: 0.696812\tvalid_1's binary_logloss: 0.0420663\tvalid_1's f1: 0.442736\n",
      "[80]\ttraining's binary_logloss: 0.0205689\ttraining's f1: 0.717639\tvalid_1's binary_logloss: 0.0408185\tvalid_1's f1: 0.454586\n",
      "[100]\ttraining's binary_logloss: 0.0194483\ttraining's f1: 0.729522\tvalid_1's binary_logloss: 0.0399014\tvalid_1's f1: 0.456239\n",
      "[120]\ttraining's binary_logloss: 0.0185689\ttraining's f1: 0.744994\tvalid_1's binary_logloss: 0.03931\tvalid_1's f1: 0.461391\n",
      "[140]\ttraining's binary_logloss: 0.0180864\ttraining's f1: 0.752068\tvalid_1's binary_logloss: 0.0390747\tvalid_1's f1: 0.466389\n",
      "[160]\ttraining's binary_logloss: 0.0176144\ttraining's f1: 0.759167\tvalid_1's binary_logloss: 0.039083\tvalid_1's f1: 0.469815\n",
      "[180]\ttraining's binary_logloss: 0.0172398\ttraining's f1: 0.763856\tvalid_1's binary_logloss: 0.0390836\tvalid_1's f1: 0.471545\n",
      "Early stopping, best iteration is:\n",
      "[138]\ttraining's binary_logloss: 0.0180837\ttraining's f1: 0.751491\tvalid_1's binary_logloss: 0.0387109\tvalid_1's f1: 0.467241\n",
      "$3 fold\n",
      "Hold out  2 month\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "[20]\ttraining's binary_logloss: 0.0284984\ttraining's f1: 0.482788\tvalid_1's binary_logloss: 0.0421124\tvalid_1's f1: 0.424076\n",
      "[40]\ttraining's binary_logloss: 0.0234268\ttraining's f1: 0.573176\tvalid_1's binary_logloss: 0.0363219\tvalid_1's f1: 0.499314\n",
      "[60]\ttraining's binary_logloss: 0.0211526\ttraining's f1: 0.614317\tvalid_1's binary_logloss: 0.0342229\tvalid_1's f1: 0.535758\n",
      "[80]\ttraining's binary_logloss: 0.0198442\ttraining's f1: 0.639905\tvalid_1's binary_logloss: 0.0331405\tvalid_1's f1: 0.550427\n",
      "[100]\ttraining's binary_logloss: 0.0189495\ttraining's f1: 0.650306\tvalid_1's binary_logloss: 0.032937\tvalid_1's f1: 0.553578\n",
      "[120]\ttraining's binary_logloss: 0.0181572\ttraining's f1: 0.666465\tvalid_1's binary_logloss: 0.0326304\tvalid_1's f1: 0.559398\n",
      "[140]\ttraining's binary_logloss: 0.0180845\ttraining's f1: 0.674959\tvalid_1's binary_logloss: 0.0326987\tvalid_1's f1: 0.563636\n",
      "[160]\ttraining's binary_logloss: 0.0172953\ttraining's f1: 0.684455\tvalid_1's binary_logloss: 0.0326982\tvalid_1's f1: 0.566702\n",
      "[180]\ttraining's binary_logloss: 0.0167967\ttraining's f1: 0.69671\tvalid_1's binary_logloss: 0.032785\tvalid_1's f1: 0.566957\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's binary_logloss: 0.0177681\ttraining's f1: 0.673857\tvalid_1's binary_logloss: 0.0325623\tvalid_1's f1: 0.563632\n",
      " F1 = 0.540965710286914\n"
     ]
    }
   ],
   "source": [
    "X = train[All_Features +['Month']]\n",
    "Y = train['fraud_ind']\n",
    "\n",
    "Oof = np.zeros(len(train))\n",
    "Preds = np.zeros(len(test))\n",
    "\n",
    "for fold_n, (train_idx, valid_idx) in enumerate(Splited_data):\n",
    "    print(f'${fold_n+1} fold')\n",
    "    X_trn, X_val= X[All_Features].iloc[train_idx], X[All_Features].iloc[valid_idx]\n",
    "    Y_trn, Y_val = Y.iloc[train_idx], Y.iloc[valid_idx]\n",
    "    print('Hold out ', X.iloc[valid_idx]['Month'].iloc[0], 'month')\n",
    "    #X_trn, Y_trn = self.Under_sample(X_trn, Y_trn)\n",
    "    \n",
    "\n",
    "\n",
    "    trn_data = lgb.Dataset(X_trn, label=Y_trn)\n",
    "    val_data = lgb.Dataset(X_val, label=Y_val)\n",
    "    \n",
    "    clf = lgb.train(params= Lgb_params, train_set= trn_data, \n",
    "                                    valid_sets= [trn_data, val_data],\n",
    "                                    num_boost_round = 300, \n",
    "                                    verbose_eval = 20, \n",
    "                                    early_stopping_rounds = 50, \n",
    "                                    categorical_feature = Cate_feat, \n",
    "                                    feval=F1_score, \n",
    "                                    evals_result={})\n",
    "    \n",
    "    Y_pred_val = clf.predict(X_val)\n",
    "    Oof[valid_idx] = Y_pred_val\n",
    "    Preds += clf.predict(test[All_Features]) / 3\n",
    "    \n",
    "Oof = np.round(Oof)\n",
    "Preds = np.round(Preds)\n",
    "Score = Cal_f1_score(Y, Oof)\n",
    "print(f' F1 = {Cal_f1_score(Y, Oof)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def Label_encoding(df, feat_list):\n",
    "    print('Initial Label_encoding.....')\n",
    "    #insfg、ecfg、ovrlt、flbmk、flg_3dsmk\n",
    "    le = LabelEncoder()\n",
    "    for feat in feat_list:\n",
    "        df[feat] = le.fit_transform(df[feat].astype(str))\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Label_encoding.....\n",
      "Initial Label_encoding.....\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "rm = gc.collect()\n",
    "del rm\n",
    "train['is_test'] = 0 \n",
    "test['is_test'] = 1 \n",
    "traub = Label_encoding(train, ['ecfg', 'flbmk', 'flg_3dsmk', 'insfg', 'ovrlt'])\n",
    "test = Label_encoding(test, ['ecfg', 'flbmk', 'flg_3dsmk', 'insfg', 'ovrlt'])\n",
    "\n",
    "\n",
    "All = pd.concat([train, test], axis=0)\n",
    "All = All\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1554761, 16)\n",
      "0.7835375658299266\n",
      "(1554761, 16)\n",
      "0.7822589151794098\n",
      "(1554762, 16)\n",
      "0.7841338856157863\n",
      "(1554762, 16)\n",
      "0.7819882168308935\n",
      "(1554762, 16)\n",
      "0.7832462888162803\n",
      "Final score :  0.7830329743158051\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "Excluded_cols = ['bacno','cano','locdt', 'loctm','OOOO_tag','scity','cano_max_locdt','cano_min_locdt',\n",
    "            'cano_stocn_nunique_locdt','cano_stocn_diff_locdt','cano_stocn_min_diff_locdt', 'txkey',\n",
    "            'cano_stocn_min_locdt','cano_stocn_max_locdt', 'fraud_ind', 'Month', 'is_test']\n",
    "All_Features = [Feat for Feat in train.columns if Feat not in Excluded_cols]\n",
    "\n",
    "skf = KFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "eval_preds = np.zeros(len(All))\n",
    "probs = np.zeros(len(All))\n",
    "\n",
    "for train_index, test_index in skf.split(All, All['is_test']):\n",
    "    x0, x1 = All[All_Features].iloc[train_index], All[All_Features].iloc[test_index]\n",
    "    y0, y1 = All['is_test'].iloc[train_index], All['is_test'].iloc[test_index]        \n",
    "    print(x0.shape)\n",
    "\n",
    "    xgb_params = {\n",
    "        'learning_rate': 0.05, 'max_depth': 4,'subsample': 0.9,\n",
    "        'objective': 'binary:logistic', 'eval_metric' : 'auc',\n",
    "        'n_estimators':3,\n",
    "        }   \n",
    "    clf = xgb.XGBClassifier(**xgb_params, seed = 10)  \n",
    "\n",
    "    \n",
    "    clf.fit(x0, y0, eval_set=[(x1, y1)],\n",
    "           eval_metric='logloss', verbose=False,early_stopping_rounds=10)\n",
    "            \n",
    "    prval = clf.predict(x1)\n",
    "    probs[test_index] = clf.predict_proba(x1)[:,1]\n",
    "    print(accuracy_score(y1,prval))\n",
    "    eval_preds[test_index] = prval\n",
    "print('Final score : ', accuracy_score(All['is_test'], eval_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All['preds'] = eval_preds\n",
    "All['probs'] = probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All.is_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "All.loc[All.is_test == 1, 'preds'].mean(), All.loc[All.is_test == 1, 'preds'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "All[All.preds == 1][['preds', 'probs']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# train      = train\n",
    "# Test_df       = test\n",
    "N_folds       = 3\n",
    "Target        = 'fraud_ind'\n",
    "\n",
    "\n",
    "def RFE(clf):\n",
    "    rfecv = RFECV(estimator=clf, step=1, cv = Split_group_kfolds(train), verbose=2, n_jobs = -1, scoring = 'f1')\n",
    "    rfecv.fit(train[All_Features], train[Target])\n",
    "    sel_features = [f for f, s in zip(All_Features, rfecv.support_) if s]\n",
    "    print('\\n The selected features are {}:'.format(sel_features))\n",
    "    print(len(sel_features))\n",
    "    \n",
    "    plt.figure(figsize=(12, 9))\n",
    "    plt.xlabel('Number of features ')\n",
    "    plt.ylabel('Cross-validation score (F1_score)')\n",
    "    plt.plot(range(1, len(rfecv.grid_scores_) + 1) , rfecv.grid_scores_)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lgb = lgb.LGBMClassifier( \n",
    "            objective = 'binary',\n",
    "            max_depth =  -1,\n",
    "            learning_rate = 0.05,\n",
    "            boosting_type = \"gbdt\",\n",
    "            bagging_freq = 4,\n",
    "#         'num_boost_round' : 100, \n",
    "        categorical_feature = Cate_feat\n",
    "                )\n",
    "\n",
    "RFE(Lgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.025, 0.001],\n",
    "    \n",
    "#     'n_estimators': [20, 40]\n",
    "    \n",
    "}\n",
    "clf = lgb.train(params= Lgb_params, train_set= trn_data, \n",
    "                valid_sets= [trn_data, val_data],\n",
    "                num_boost_round = 5, \n",
    "                verbose_eval = 100, \n",
    "                early_stopping_rounds = 50, \n",
    "                categorical_feature = Cate_feat, \n",
    "                feval=F1_score, \n",
    "                evals_result={})\n",
    "\n",
    "gbm = GridSearchCV(estimator, param_grid, cv=3)\n",
    "gbm.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
