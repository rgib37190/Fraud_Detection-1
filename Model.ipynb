{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import gc\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import pandas as pd\n",
    "import os \n",
    "from datetime import datetime\n",
    "from Tuning_parameter import Lgb_params\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import xgboost as xgb\n",
    "import scipy\n",
    "\n",
    "#from Tuning_parameter import Lgb_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1969"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#id : 'bacno', 'cano', 'txkey'\n",
    "#continuous : 'conam', 'iterm'\n",
    "#time : 'locdt', 'loctm'\n",
    "#binary : 'ecfg', 'insfg', 'ovrlt'\n",
    "#multi category :'acqic', 'contp', 'csmcu', 'etymd',\n",
    "#       'flbmk', 'flg_3dsmk', 'hcefg', \n",
    "#     'mcc', 'mchno', 'scity', 'stocn', 'stscd',\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rand_under_sam = RandomUnderSampler(random_state=31)\n",
    "# def sample(df):\n",
    "#     Sampled_df = pd.DataFrame()\n",
    "#     for i in range(1, 3):\n",
    "#         Temp_df = df[df['Month'] == i]\n",
    "#         Sample_X, Sample_y = Rand_under_sam.fit_resample(Temp_df[['txkey']], Temp_df['fraud_ind'])\n",
    "#         Sample_X = pd.DataFrame(Sample_X, columns= ['txkey'])\n",
    "#         Sample_y = pd.DataFrame(Sample_y, columns= ['fraud_ind'])\n",
    "#         Sample_X = pd.merge(Sample_X, Temp_df, on ='txkey', how ='inner')\n",
    "#         Sampled_df = pd.concat([Sampled_df, Sample_X], axis = 0)\n",
    "#     return Sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean_conam ,"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/ipykernel_launcher.py:20: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stocn_scity_FE , stocn_scity , stocn_scity_cano , stocn_scity_cano , conam_iterm , conam_iterm , Month_bacno , Month_bacno , Month_cano , Month_cano , 'conam_bacno_mean' , 'conam_bacno_std' , 'conam_bacno_max' , 'conam_bacno_median' , 'conam_bacno_nunique' , 'conam_bacno_min' , 'conam_bacno_sum' , 'conam_cano_mean' , 'conam_cano_std' , 'conam_cano_max' , 'conam_cano_median' , 'conam_cano_nunique' , 'conam_cano_min' , 'conam_cano_sum' , 'conam_Month_bacno_mean' , 'conam_Month_bacno_std' , 'conam_Month_bacno_max' , 'conam_Month_bacno_median' , 'conam_Month_bacno_nunique' , 'conam_Month_bacno_min' , 'conam_Month_bacno_sum' , 'conam_Month_cano_mean' , 'conam_Month_cano_std' , 'conam_Month_cano_max' , 'conam_Month_cano_median' , 'conam_Month_cano_nunique' , 'conam_Month_cano_min' , 'conam_Month_cano_sum' , 'iterm_bacno_mean' , 'iterm_bacno_std' , 'iterm_bacno_max' , 'iterm_bacno_median' , 'iterm_bacno_nunique' , 'iterm_bacno_min' , 'iterm_bacno_sum' , 'iterm_cano_mean' , 'iterm_cano_std' , 'iterm_cano_max' , 'iterm_cano_median' , 'iterm_cano_nunique' , 'iterm_cano_min' , 'iterm_cano_sum' , 'iterm_Month_bacno_mean' , 'iterm_Month_bacno_std' , 'iterm_Month_bacno_max' , 'iterm_Month_bacno_median' , 'iterm_Month_bacno_nunique' , 'iterm_Month_bacno_min' , 'iterm_Month_bacno_sum' , 'iterm_Month_cano_mean' , 'iterm_Month_cano_std' , 'iterm_Month_cano_max' , 'iterm_Month_cano_median' , 'iterm_Month_cano_nunique' , 'iterm_Month_cano_min' , 'iterm_Month_cano_sum' , 'Mean_conam_bacno_mean' , 'Mean_conam_bacno_std' , 'Mean_conam_bacno_max' , 'Mean_conam_bacno_median' , 'Mean_conam_bacno_nunique' , 'Mean_conam_bacno_min' , 'Mean_conam_bacno_sum' , 'Mean_conam_cano_mean' , 'Mean_conam_cano_std' , 'Mean_conam_cano_max' , 'Mean_conam_cano_median' , 'Mean_conam_cano_nunique' , 'Mean_conam_cano_min' , 'Mean_conam_cano_sum' , 'Mean_conam_Month_bacno_mean' , 'Mean_conam_Month_bacno_std' , 'Mean_conam_Month_bacno_max' , 'Mean_conam_Month_bacno_median' , 'Mean_conam_Month_bacno_nunique' , 'Mean_conam_Month_bacno_min' , 'Mean_conam_Month_bacno_sum' , 'Mean_conam_Month_cano_mean' , 'Mean_conam_Month_cano_std' , 'Mean_conam_Month_cano_max' , 'Mean_conam_Month_cano_median' , 'Mean_conam_Month_cano_nunique' , 'Mean_conam_Month_cano_min' , 'Mean_conam_Month_cano_sum' , 'cano_bacno_nunique' , 'cano_bacno_count' , 'txkey_bacno_count' , 'txkey_Month_bacno_count' , 'txkey_Month_cano_count' , 'txkey_cano_count' , contp_FE , etymd_FE , hcefg_FE , stscd_FE , acqic , csmcu , mcc , mchno , scity , stocn , 'acqic_bacno_nunique' , 'acqic_bacno_count' , 'acqic_cano_nunique' , 'acqic_cano_count' , 'csmcu_bacno_nunique' , 'csmcu_bacno_count' , 'csmcu_cano_nunique' , 'csmcu_cano_count' , 'mcc_bacno_nunique' , 'mcc_bacno_count' , 'mcc_cano_nunique' , 'mcc_cano_count' , 'mchno_bacno_nunique' , 'mchno_bacno_count' , 'mchno_cano_nunique' , 'mchno_cano_count' , 'scity_bacno_nunique' , 'scity_bacno_count' , 'scity_cano_nunique' , 'scity_cano_count' , 'stocn_bacno_nunique' , 'stocn_bacno_count' , 'stocn_cano_nunique' , 'stocn_cano_count' , 'stocn_scity_bacno_nunique' , 'stocn_scity_bacno_count' , 'stocn_scity_cano_nunique' , 'stocn_scity_cano_count' , "
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Cannot access callable attribute 'mode' of 'SeriesGroupBy' objects, try using the 'apply' method",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-1313f9dd2aff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'stscd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flg_3dsmk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flbmk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'flg_3dsmk'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'contp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'hcefg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_mode_gb_bacno'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bacno'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'_mode_gb_cano'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cano'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0;31m# cythonized aggregation and merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 return self._transform_fast(\n\u001b[0;32m-> 1017\u001b[0;31m                     \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m                 )\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_transform_fast\u001b[0;34m(self, func, func_nm)\u001b[0m\n\u001b[1;32m   1062\u001b[0m         \u001b[0mids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mngroup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0mcast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_should_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_nm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1064\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1065\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_cast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1015\u001b[0m                 \u001b[0;31m# cythonized aggregation and merge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m                 return self._transform_fast(\n\u001b[0;32m-> 1017\u001b[0;31m                     \u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m                 )\n\u001b[1;32m   1019\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    561\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m         raise AttributeError(\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_make_wrapper\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    602\u001b[0m                 \u001b[0;34m\"using the 'apply' method\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m             )\n\u001b[0;32m--> 604\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_group_selection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Cannot access callable attribute 'mode' of 'SeriesGroupBy' objects, try using the 'apply' method"
     ]
    }
   ],
   "source": [
    "def Process_loctm(df1, df2):\n",
    "    def Str_turn_time(str1):\n",
    "        str1 = str(int(str1))\n",
    "        if len(str1) < 6:\n",
    "            str1 = (6 - len(str1)) * '0' + str1\n",
    "        return str1\n",
    "    \n",
    "    df_comb = pd.concat([df1, df2],axis=0)\n",
    "    \n",
    "    df_comb['Hour'] = df_comb['loctm'].apply(lambda x :Str_turn_time(x)[:2]).astype(int)\n",
    "    df_comb['Morning'] = 0\n",
    "    df_comb.loc[(df_comb['Hour'].astype('int') > 7) & (df_comb['Hour'].astype('int') < 22), 'Morning'] = 1\n",
    "    \n",
    "    df1 = df_comb[:len(df1)]\n",
    "    df2 = df_comb[len(df1):]\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "def Separate_dt(df1, df2):\n",
    "    df_comb = pd.concat([df1, df2],axis=0)\n",
    "    \n",
    "    df_comb['Month'] = 0\n",
    "    df_comb.loc[df_comb['locdt'] < 121, 'Month'] = 4\n",
    "    df_comb.loc[df_comb['locdt'] < 91, 'Month']  = 3\n",
    "    df_comb.loc[df_comb['locdt'] < 61, 'Month']  = 2\n",
    "    df_comb.loc[df_comb['locdt'] < 31, 'Month'] = 1\n",
    "    \n",
    "    df1 = df_comb[:len(df1)]\n",
    "    df2 = df_comb[len(df1):]\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# FREQUENCY ENCODE TOGETHER\n",
    "def encode_FE(df1, df2, cols, replace=False):\n",
    "    if replace:\n",
    "        for col in cols:\n",
    "            df = pd.concat([df1[col],df2[col]])\n",
    "            vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "            vc[-1] = -1\n",
    "            df1[col] = df1[col].map(vc)\n",
    "            df2[col] = df2[col].map(vc)\n",
    "            print(col,', ',end='')\n",
    "    else:\n",
    "        for col in cols:\n",
    "            df = pd.concat([df1[col],df2[col]])\n",
    "            vc = df.value_counts(dropna=True, normalize=True).to_dict()\n",
    "            vc[-1] = -1\n",
    "            new_col = col+'_FE'\n",
    "            df1[new_col] = df1[col].map(vc)\n",
    "            df2[new_col] = df2[col].map(vc)\n",
    "            print(new_col,', ',end='')\n",
    "            \n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "# LABEL ENCODE\n",
    "def encode_LE(df1, df2, col):\n",
    "    df_comb = pd.concat([train[col],test[col]],axis=0)\n",
    "    df_comb,_ = df_comb.factorize(sort=True)\n",
    "\n",
    "    if df_comb.max()>32000: \n",
    "        train[col] = df_comb[:len(train)].astype('int32')\n",
    "        test[col] = df_comb[len(train):].astype('int32')\n",
    "    else:\n",
    "        train[col] = df_comb[:len(train)].astype('int16')\n",
    "        test[col] = df_comb[len(train):].astype('int16')\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "# COMBINE FEATURES\n",
    "def encode_CB(df1, df2, col1, col2, replace = True):\n",
    "    new_col = col1+'_'+col2\n",
    "    df1[new_col] = df1[col1].astype(str)+'_'+df1[col2].astype(str)\n",
    "    df2[new_col] = df2[col1].astype(str)+'_'+df2[col2].astype(str) \n",
    "    df1, df2 = encode_FE(df1, df2, [new_col], replace = replace)\n",
    "\n",
    "    print(new_col,', ',end='')\n",
    "    \n",
    "    return df1, df2\n",
    "\n",
    "\n",
    "def encode_AG(main_columns, uids, aggregations, df1, df2, fillna=True, usena=False):\n",
    "    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n",
    "    for main_column in main_columns:  \n",
    "        for col in uids:\n",
    "            if main_column != col:\n",
    "                for agg_type in aggregations:\n",
    "                    new_col_name = main_column+'_'+col+'_'+agg_type\n",
    "                    temp_df = pd.concat([df1[[col, main_column]], df2[[col,main_column]]])\n",
    "                    \n",
    "                    if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n",
    "                    temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n",
    "                                                            columns={agg_type: new_col_name})\n",
    "    \n",
    "                    temp_df.index = list(temp_df[col])\n",
    "                    temp_df = temp_df[new_col_name].to_dict()   \n",
    "    \n",
    "                    df1[new_col_name] = df1[col].map(temp_df).astype('float32')\n",
    "                    df2[new_col_name]  = df2[col].map(temp_df).astype('float32')\n",
    "                    \n",
    "                    if fillna:\n",
    "                        df1[new_col_name].fillna(-1,inplace=True)\n",
    "                        df2[new_col_name].fillna(-1,inplace=True)\n",
    "                    \n",
    "                    print(\"'\"+new_col_name+\"'\",', ',end='')\n",
    "    return df1, df2   \n",
    "\n",
    "def Set_cate_cols(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df1[col] = df1[col].astype('category')\n",
    "        df2[col] = df2[col].astype('category')\n",
    "    return df1, df2\n",
    "\n",
    "def Fill_na(df1, df2, cols):\n",
    "    for col in cols:\n",
    "        df1[col] = df1[col].fillna(-99)\n",
    "        df2[col] = df2[col].fillna(-99)\n",
    "    return df1, df2\n",
    "\n",
    "def F1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "\n",
    "def f1_eval(y_pred, dtrain):\n",
    "    y_true = dtrain.get_label()\n",
    "    return 'f1_score', f1_score(y_true, np.round(y_pred))\n",
    "\n",
    "def get_mode(x):\n",
    "    return scipy.stats.mode(x.values)[0][0]\n",
    "\n",
    "less_cate_cols = ['contp', 'etymd', 'hcefg', 'stscd']\n",
    "more_cate_cols = ['acqic', 'csmcu', 'mcc', 'mchno', 'scity', 'stocn']\n",
    "#cate_cols = ['acqic', 'contp', 'csmcu', 'etymd', 'hcefg', \n",
    "#     'mcc', 'mchno', 'scity', 'stocn', 'stscd']\n",
    "\n",
    "#FE\n",
    "train['Mean_conam'] = (train['conam']+0.001) / (train['iterm'] +1)\n",
    "test['Mean_conam']  = (test['conam']+0.001) / (test['iterm'] +1)\n",
    "\n",
    "\n",
    "# train['Both_0'] = 0\n",
    "train.loc[(train['conam'] == 0) & (train['iterm'] == 0), 'Both_0'] = 1\n",
    "test['Both_0'] = 0\n",
    "test.loc[(test['conam'] == 0) & (test['iterm'] == 0), 'Both_0'] = 1\n",
    "\n",
    "print(\"Mean_conam\", ',', end='')\n",
    "train, test = Separate_dt(train, test)\n",
    "#fill na\n",
    "train, test = Fill_na(train, test, ['flbmk', 'flg_3dsmk'])\n",
    "\n",
    "#Calculate time \n",
    "train, trst = Process_loctm(train, test)\n",
    "\n",
    "train, test = encode_LE(train, test, 'insfg')\n",
    "train, test = encode_LE(train, test, 'ecfg')\n",
    "train, test = encode_LE(train, test, 'ovrlt')\n",
    "train, test = encode_LE(train, test, 'flbmk')\n",
    "train, test = encode_LE(train, test, 'flg_3dsmk')\n",
    "\n",
    "#CB\n",
    "train, test = encode_CB(train, test, 'stocn', 'scity', False)\n",
    "train, test = encode_CB(train, test, 'stocn_scity', 'cano', True)\n",
    "train, test = encode_CB(train, test, 'conam', 'iterm', True)\n",
    "train, test = encode_CB(train, test, 'Month', 'bacno', True)\n",
    "train, test = encode_CB(train, test, 'Month', 'cano', True)\n",
    "\n",
    "#AG \n",
    "train, test = encode_AG(['conam', 'iterm', 'Mean_conam'], ['bacno', 'cano', 'Month_bacno', 'Month_cano'], ['mean', 'std', 'max', 'median', 'nunique', 'min', 'sum'], train, test)\n",
    "train, test = encode_AG(['cano'], ['bacno'], ['nunique', 'count'], train, test)\n",
    "train, test = encode_AG(['txkey'], ['bacno'], ['count'], train, test)\n",
    "train, test = encode_AG(['txkey'], ['Month_bacno'], ['count'], train, test)\n",
    "train, test = encode_AG(['txkey'], ['Month_cano'], ['count'], train, test)\n",
    "train, test = encode_AG(['txkey'], ['cano'], ['count'], train, test)\n",
    "\n",
    "#interection btw 'cano', 'bacno'\n",
    "train['cnt_ratio_by_cano_bacno'] = train['txkey_cano_count'] / train['txkey_bacno_count'] \n",
    "test['cnt_ratio_by_cano_bacno'] = test['txkey_cano_count'] / test['txkey_bacno_count'] \n",
    "\n",
    "train['conam_ratio_by_cano_bacno'] = train['conam_cano_sum'] / train['conam_bacno_sum'] \n",
    "train['conam_ratio_by_bacno'] = train['conam'] / train['conam_bacno_sum'] \n",
    "train['conam_ratio_by_cano'] = train['conam'] / train['conam_cano_sum'] \n",
    "\n",
    "test['conam_ratio_by_cano_bacno'] = test['conam_cano_sum'] / test['conam_bacno_sum'] \n",
    "test['conam_ratio_by_bacno'] = test['conam'] / test['conam_bacno_sum'] \n",
    "test['conam_ratio_by_cano'] = test['conam'] / test['conam_cano_sum'] \n",
    "\n",
    "\n",
    "train, test = encode_FE(train, test, less_cate_cols, replace = False)\n",
    "train, test = encode_FE(train, test, more_cate_cols, replace = True)\n",
    "\n",
    "#set cate_feautre\n",
    "train, test = Set_cate_cols(train, test, ['stscd', 'flg_3dsmk', 'flbmk', 'flg_3dsmk', 'contp', 'hcefg'])\n",
    "\n",
    "train, test = encode_AG(more_cate_cols + ['stocn_scity'], ['bacno', 'cano'], ['nunique', 'count'], train, test)\n",
    "\n",
    "# for col in ['stscd', 'flg_3dsmk', 'flbmk', 'flg_3dsmk', 'contp', 'hcefg']:\n",
    "#     train[col +'_mode_gb_bacno'] = train.groupby('bacno')[col].transform('mode')\n",
    "#     test[col +'_mode_gb_cano']  = test.groupby('cano')[col].transform('mode')\n",
    "\n",
    "\n",
    "train.drop(['stocn_scity'], axis = 1, inplace = True)\n",
    "test.drop(['stocn_scity'], axis = 1, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['stscd', 'flg_3dsmk', 'flbmk', 'flg_3dsmk', 'contp', 'hcefg'] + ['Hour']:\n",
    "    train[col +'_mode_gb_bacno'] = train.groupby('bacno')[col].transform(get_mode)\n",
    "    test[col +'_mode_gb_cano']  = test.groupby('cano')[col].transform(get_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total columns :  160\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#id : 'bacno', 'cano', 'txkey'\n",
    "#continuous : 'conam', 'iterm'\n",
    "#time : 'locdt', 'loctm'\n",
    "#binary : 'ecfg', 'insfg', 'ovrlt'\n",
    "#multi category :'acqic', 'contp', 'csmcu', 'etymd',\n",
    "#       'flbmk', 'flg_3dsmk', 'hcefg', \n",
    "#     'mcc', 'mchno', 'scity', 'stocn', 'stscd',\n",
    "\n",
    "SEED = 42\n",
    "Excluded_cols = ['fraud_ind', 'txkey', 'Month', 'cano']\n",
    "traning_cols = [col for col in train.columns if col not in Excluded_cols]\n",
    "print('Total columns : ', len(traning_cols))\n",
    "\n",
    "train_df = train[train.Month != 3]\n",
    "val_df   = train[train.Month == 3]\n",
    "test_df  = test\n",
    "\n",
    "trn_data = lgb.Dataset(train_df[traning_cols], label=train_df['fraud_ind'])\n",
    "val_data = lgb.Dataset(val_df[traning_cols], label=val_df['fraud_ind'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lgb_params = {\n",
    "        'num_leaves': 32,\n",
    "        'min_child_samples': 5,\n",
    "        'objective': 'binary',\n",
    "        'max_depth': -1,\n",
    "        'learning_rate': 0.05,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        #\"subsample\": 0.8,\n",
    "        \"bagging_seed\": 11,\n",
    "        #\"verbosity\": -1,\n",
    "        'reg_alpha': 0.3,\n",
    "        'reg_lambda': 0.3,\n",
    "        #'colsample_bytree': 0.8,\n",
    "        'num_boost_round' : 500, \n",
    "        'verbose_eval' : 100,\n",
    "        'early_stopping_rounds' : 50,\n",
    "        #'scale_pos_weight' : 700,\n",
    "        'num_threads' :6,\n",
    "        #'scale_pos_weight' :50\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's binary_logloss: 0.0241269\ttraining's f1: 0.648895\tvalid_1's binary_logloss: 0.0316941\tvalid_1's f1: 0.353643\n",
      "[100]\ttraining's binary_logloss: 0.0198374\ttraining's f1: 0.720539\tvalid_1's binary_logloss: 0.0300651\tvalid_1's f1: 0.373037\n",
      "[150]\ttraining's binary_logloss: 0.0177965\ttraining's f1: 0.754839\tvalid_1's binary_logloss: 0.0295321\tvalid_1's f1: 0.377416\n",
      "Early stopping, best iteration is:\n",
      "[123]\ttraining's binary_logloss: 0.0187703\ttraining's f1: 0.738762\tvalid_1's binary_logloss: 0.0297161\tvalid_1's f1: 0.382926\n"
     ]
    }
   ],
   "source": [
    "Lgb = lgb.train(\n",
    "            Lgb_params,\n",
    "            trn_data,\n",
    "            valid_sets = [trn_data, val_data],\n",
    "            verbose_eval = 50,\n",
    "            feval=F1_score, \n",
    "            evals_result={}\n",
    "        )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_df = pd.DataFrame()\n",
    "importance_df['feature']    = traning_cols\n",
    "importance_df['importance'] = np.log1p(Lgb.feature_importance(importance_type='gain', iteration=Lgb.best_iteration))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Mean_conam</td>\n",
       "      <td>11.076542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>acqic</td>\n",
       "      <td>10.107453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bacno</td>\n",
       "      <td>10.419316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>conam</td>\n",
       "      <td>10.465919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>contp</td>\n",
       "      <td>3.749843</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature  importance\n",
       "0  Mean_conam   11.076542\n",
       "1       acqic   10.107453\n",
       "2       bacno   10.419316\n",
       "3       conam   10.465919\n",
       "4       contp    3.749843"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importance_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.round(Lgb.predict(test_df[traning_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Xgb = xgb.XGBClassifier( \n",
    "        n_estimators=1000,\n",
    "        max_depth=12, \n",
    "        learning_rate=0.5, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        #missing= 'NAN', \n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist' \n",
    "        # USE GPU\n",
    "    )\n",
    "Xgb.fit(train_df[traning_cols], train_df['fraud_ind'], \n",
    "        eval_set=[(val_df[traning_cols], val_df['fraud_ind'])],\n",
    "        verbose=100, early_stopping_rounds=200, eval_metric = f1_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.flbmk.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Train_df      = Processed_data['Train_df']\n",
    "Test_df       = Processed_data['Test_df']\n",
    "Test_id       = Processed_data['Test_df']['txkey']\n",
    "#Record_list   = Processed_data['Record_list']\n",
    "Cate_feat     = Processed_data['Cate_feat']\n",
    "\n",
    "N_folds       = Processed_data['N_folds']\n",
    "Target        = 'fraud_ind'\n",
    "#Time          = datetime.today().strftime('%m-%d-%H-%M')\n",
    "\n",
    "def Cal_f1_score(True_y, Pre_y):\n",
    "    return f1_score(True_y, Pre_y)\n",
    "            \n",
    "def F1_score(y_hat, data):\n",
    "    y_true = data.get_label()\n",
    "    y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n",
    "    return 'f1', f1_score(y_true, y_hat), True\n",
    "\n",
    "def Split_group_kfolds():\n",
    "    Train_X = Train_df.drop(['fraud_ind'], axis=1)\n",
    "    print(Train_X['Month'].unique())\n",
    "    Train_Y = Train_df['fraud_ind']\n",
    "    Folds  = GroupKFold(n_splits=3)\n",
    "    Splited_data = Folds.split(Train_X, Train_Y, groups=Train_X['Month']) \n",
    "    return Splited_data\n",
    "\n",
    "\n",
    "clf = lgb.LGBMClassifier( \n",
    "                #early_stopping_rounds = 10, \n",
    "                #categorical_feature = Cate_feat, \n",
    "                #feval=F1_score, \n",
    "                #evals_result={},     \n",
    "                num_leaves =  31,\n",
    "                min_child_samples =  79,\n",
    "                objective = 'binary',\n",
    "                max_depth = -1,\n",
    "                learning_rate =  0.025,\n",
    "                boosting_type = 'gbdt',\n",
    "                #\"subsample\": 0.8,\n",
    "                bagging_seed = 42,\n",
    "                #\"verbosity\": -1,\n",
    "                #'reg_alpha': 0.3,\n",
    "                #'reg_lambda': 0.3,\n",
    "                #'colsample_bytree': 0.8,\n",
    "                n_estimators = 1, \n",
    "                #verbose_eval = 100,\n",
    "                #'scale_pos_weight' : 700,\n",
    "                #num_threads  =4\n",
    "                )\n",
    "\n",
    "rfecv = RFECV(estimator=clf, step=1, cv = Split_group_kfolds(), verbose=2, n_jobs = -1, scoring = 'f1')\n",
    "\n",
    "Exluded_cols = ['fraud_ind', 'txkey', 'Month']\n",
    "All_Features = [Feat for Feat in Train_df.columns if Feat not in Exluded_cols]\n",
    "\n",
    "rfecv.fit(Train_df[All_Features], Train_df[Target])\n",
    "sel_features = [f for f, s in zip(All_Features, rfecv.support_) if s]\n",
    "print('\\n The selected features are {}:'.format(sel_features))\n",
    "print(len(sel_features))\n",
    "\n",
    "plt.figure(figsize=(12, 9))\n",
    "plt.xlabel('Number of features ')\n",
    "plt.ylabel('Cross-validation score (F1_score)')\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1) , rfecv.grid_scores_)\n",
    "plt.show()\n",
    "\n",
    "print((time.time() - Start_time)/60, 'minute')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xgb = xgb.XGBClassifier( \n",
    "        n_estimators=1000,\n",
    "        max_depth=-1, \n",
    "        learning_rate=0.02, \n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.4, \n",
    "        missing=-1, \n",
    "        eval_metric='f1_score',\n",
    "        # USE CPU\n",
    "        #nthread=4,\n",
    "        #tree_method='hist' \n",
    "        # USE GPU\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
